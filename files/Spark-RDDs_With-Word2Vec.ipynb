{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Understanding Word2Vec with PySpark</center>\n",
    "<center>Gabriel Fair</center>\n",
    "\n",
    "\n",
    "Today we are going to look at how Word2Vec incorporates word embeddings to create a numeric vectors to represent meaning of words. This is an important part of natural language processing (NLP). The goal of NLP is to extract meaning from human language, often this is provided in the form of text. And this meaning can be found in many components of language.\n",
    "\n",
    "### Some components of language\n",
    "  - Pragmatics\n",
    "  - Semantics\n",
    "  - Syntax\n",
    "  - Morphology\n",
    "  - Phonology\n",
    "\n",
    "## Distributional Semantic Models\n",
    "Word embeddings are word representation algorithms used in an NLP. Word embeddings are a subclass of distributional semantic models because they rely on the distributional hypothesis. The distributional hypothesis, created by Zellig Harris in his 1956 paper [“Distributional structure”](http://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520) ,  is assumption that words in the same context tend to proport similar meanings, and thus occur near each other. And thus synonyms have similar representations in a collection of texts. Word embeddings are represented as vector values created as a result of a neural network. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "<img style=\"display: block; margin: auto;\" alt=\"photo\" src=\"{{ site.baseurl }}/images/image.jpg\">\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are going to start with some imports that we will need later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark version and install location: 2.4.0\n",
      "<module 'pyspark.version' from '/opt/spark-2.4.0-bin-hadoop2.7/python/pyspark/version.py'>\n"
     ]
    }
   ],
   "source": [
    "import os, sys, codecs, json, datetime\n",
    "\n",
    "from time import time\n",
    "import pyspark\n",
    "print(\"pyspark version and install location: \" + str(sc.version))\n",
    "print(str(pyspark.version))\n",
    "\n",
    "\n",
    "from pyspark.mllib.feature import Word2Vec as Word2Vec #https://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html#word2vec\n",
    "from pyspark.mllib.clustering import KMeans as KMeans\n",
    "from pyspark.mllib.linalg import Vectors as Vectors\n",
    "\n",
    "#https://spark.apache.org/docs/2.2.0/ml-features.html#word2vec\n",
    "from pyspark.ml.feature import Word2Vec as Word2Vec2 #https://spark.apache.org/docs/2.2.0/ml-feature-extraction.html#word2vec\n",
    "from pyspark.ml.clustering import KMeans as KMeans2\n",
    "from pyspark.ml.linalg import Vectors as Vectors2\n",
    "\n",
    "#from pyspark.ml.feature import PCA\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine which version of scikit-learn learn we are using. \n",
    "#### [As of version 0.20.0 scikit-learn supports Pandas dataframes. ](https://medium.com/dunder-data/from-pandas-to-scikit-learn-a-new-exciting-workflow-e88e2271ef62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn's version is: 0.20.1\n",
      "My python version is: 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) \n",
      "[GCC 7.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(\"sklearn's version is: \" + str(sklearn.__version__))\n",
    "print(\"My python version is: \"+ str(sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A method to monitor progress\n",
    "def update_progress(current_progress, total, current):\n",
    "    text = str(current_progress) + \"/\" + str(total) + \" At: \" + str(current)\n",
    "    sys.stdout.write('\\r' + text)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input 1 Thousand gab.ai posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are posts total: 1000001\n"
     ]
    }
   ],
   "source": [
    "path_to_text_data = '1mill_posts_unique_body_only.csv'\n",
    "#filter_words = 'filter_word.csv' #Needs to one word per line\n",
    "\n",
    "post_data = sc.textFile(path_to_text_data)\n",
    "totalposts = post_data.count()\n",
    "print(\"There are posts total: \" + str(totalposts) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example gab.ai posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.body\n",
      "\"Probably because I see the faint hint of 'horns' holding that halo up... \"\n",
      "https://youtu.be/YMQRFT4bZuc\n",
      "http://www.epochtimes.de/politik/europa/zahl-der-toten-nach-londoner-hochhausbrand-auf-79-gestiegen-2-a2146594.html\n",
      "https://t.co/LTMBeXvHrC\n",
      "\"Ps 37:14 Die Gottlosen ziehen das Schwert aus und spannen ihren Bogen, daß sie fällen den Elenden und Armen und schlachten die Frommen.\\nPs 37:15 Aber ihr Schwert wird in ihr Herz gehen, und ihr Bogen wird zerbrechen.\\n\\n\"\n",
      "At least 25 killed in airstrike on market in Yemen – reports\\nhttps://www.rt.com/news/392838-saudi-yemen-market-airstrike/ #saudiarabia #yemen\n"
     ]
    }
   ],
   "source": [
    "for i, y in enumerate(post_data.collect()):\n",
    "    print(y)\n",
    "    if i >5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean characters by removing some characters and transform text to lower case\n",
    "posts_RDD = post_data.map(lambda x: x.replace(\";\",\" \").replace(\":\",\" \").replace('\"',' ').replace('-',' ').replace(',',' ').replace('.',' ').lower())\n",
    "\n",
    "# tokenize into separate words\n",
    "posts_RDD = posts_RDD.map(lambda row: row.split(\" \")) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'body']\n",
      "['', 'probably', 'because', 'i', 'see', 'the', 'faint', 'hint', 'of', \"'horns'\", 'holding', 'that', 'halo', 'up', '', '', '', '', '']\n",
      "['https', '//youtu', 'be/ymqrft4bzuc']\n",
      "['http', '//www', 'epochtimes', 'de/politik/europa/zahl', 'der', 'toten', 'nach', 'londoner', 'hochhausbrand', 'auf', '79', 'gestiegen', '2', 'a2146594', 'html']\n",
      "['https', '//t', 'co/ltmbexvhrc']\n",
      "['', 'ps', '37', '14', 'die', 'gottlosen', 'ziehen', 'das', 'schwert', 'aus', 'und', 'spannen', 'ihren', 'bogen', '', 'daß', 'sie', 'fällen', 'den', 'elenden', 'und', 'armen', 'und', 'schlachten', 'die', 'frommen', '\\\\nps', '37', '15', 'aber', 'ihr', 'schwert', 'wird', 'in', 'ihr', 'herz', 'gehen', '', 'und', 'ihr', 'bogen', 'wird', 'zerbrechen', '\\\\n\\\\n', '']\n",
      "['at', 'least', '25', 'killed', 'in', 'airstrike', 'on', 'market', 'in', 'yemen', '–', 'reports\\\\nhttps', '//www', 'rt', 'com/news/392838', 'saudi', 'yemen', 'market', 'airstrike/', '#saudiarabia', '#yemen']\n"
     ]
    }
   ],
   "source": [
    "for i, y in enumerate(posts_RDD.collect()):\n",
    "    print(y) #There is no need to worry about the blank string elements, they will be ignored anyway\n",
    "    if i >5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Vector\n",
    "To use the distributional hypothesis to build a vector, we have to choose what words being near each other means to us. This value of “nearness” is known as a window. In the image below, taken from [Chris McCormick’s tutorial](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/), the target word is highlighted in blue, and the window shown around it as being two words away from the target.\n",
    "This means the window size is equal to 2.\n",
    "<img style=\"display: block; margin: auto;\" alt=\"photo\" src=\"http://mccormickml.com/assets/word2vec/training_data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word pairs are created between the target word and  all other words in the window which can extend forwards or backwards. The target is then moved to the next word and the process repeats. Some embedding models treat words to the left of the target word differently than words to the right. But for now we will treat them both equally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These word pairs become the training samples for the model. These pairs are known as a one-hot vector. Currently they are in the form of (target word, context-word-in-window) and this will be used as the input for a simple 1 hidden layer neural network. The hidden layer has a pre-determined number of neurons that we specify as a hyper parameter. A hyper parameter is the number of hidden layer neurons has a large effect on the accacury and speed of the model’s runtime and 300 is widely used in practice since it was used by word2vec’s creators. This simple neural network is known as a Restricted Boltzmann Machine (RBM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricted Boltzmann Machines (RBMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"display: block; margin: auto;\" alt=\"photo\" src=\"https://raw.githubusercontent.com/gabefair/gabefair.github.io/master/images/threelayers%5B1%5D.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image above, there are three columns that are known in discriptions of neural networks as layers. These diagrams show cause and effect between the layers of a neural network and are read from left-to-right. Each circle represents a neuron and is called a **node**. A node is where a calculation is preformed to determine if it will send a 0 or a 1 to a node in the next layer, which is to the right. This communication is known as **firing** and only happens in one direction, left-to-right. \n",
    "\n",
    "\n",
    "<img style=\"display: block; margin: auto;\" alt=\"photo\" src=\"https://raw.githubusercontent.com/gabefair/gabefair.github.io/master/images/three_layers_connected.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our restricted boltzman machine, nodes are not linked to, or communicate with, other nodes within the same layer. This restriction gives the RBM its name. And every node in the input layer is linked to each node in the hidden layer. The nodes/neurons in the input layer are considered to be different neurons  in the hidden layer, hence why they are in different layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I stress this point because this is known as a bipartite graph. But not just any bipartide graph, a complete bipartite graph because these two layers are fully linked. Note, that some texts call this a symmetrical bipartite graph. Also it is important to notice in the graphic above how the hidden layer has fewer nodes than the input or output layers. This is an important quality of RBMs as a feature known as dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a RBM is inalitized, four things are determined in advance and thus are hard-coded into the construction of the neural network. This things are known as hyper parameters. \n",
    "\n",
    "  - Number of nodes in the input layer\n",
    "  - Number of nodes in the hidden layer\n",
    "  - Number of nodes in the output layer\n",
    "  - The weights of the nodes in the hidden layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With RBMs a special step happens when the hidden layer is created. Each node is randomlly assigned a weight. A wight is the power that node has on the nodes it is linked to in the next layer. This process of randomlly assigning weights is known as Stochastic Gradient Descent. It is called this because stocastic means “random” and these weights provide influence on the node in the next layer they are linked to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weights are important to Word2Vec but unlike normal RBMs, Word2Vec does not randomly assign weights. Instead Word2Vec builts these weights over time while the neural network is fed our word pairs we created previously as input. This is known as training the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and train the neural network used by word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec model param: wordIndex maps each word to an index, \n",
    "# which can retrieve the corresponding vector from wordVectors \n",
    "# param: wordVectors array of length numWords * vectorSize, vector corresponding to the word mapped with index i can be retrieved by the slice (i * vectorSize, i * vectorSize + vectorSize)\n",
    "\n",
    "k = 300         # vector dimensionality (The number of nodes in the hidden layer)\n",
    "word2vec = Word2Vec().setVectorSize(k) #Uses skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train Word2vec\n",
    "#dir(word2vec)\n",
    "model = wc.fit(posts_RDD)\n",
    "\n",
    "model_vectors = model.getVectors()\n",
    "## Get the list of words in the w2v matrix\n",
    "vocabsize = len(model_vectors)\n",
    "print(\"Size of vocab list: \" + str(vocabsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The the results of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(model_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just the output vector for the word \"trump\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_vectors['trump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocab list: 1027\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vector for the word 'looks': 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of vector for the word 'looks': \" + str(len(a['looks'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_look_up_final_score = 'trump'\n",
    "another_word_to_compare_it_against = 'president'\n",
    "\n",
    "#Find synonyms of a word; do not include the word itself in results\n",
    "word_cosine_similarity_arry_word1 = model.findSynonyms(word_to_look_up_final_score, vocabsize-1) #returns an array of (word, cosineSimilarity)\n",
    "word_cosine_similarity_arry_word2 = model.findSynonyms(another_word_to_compare_it_against, vocabsize-1) #returns an array of (word, cosineSimilarity)\n",
    "\n",
    "list_words = []\n",
    "for l in word_cosine_similarity_arry_word1:\n",
    "    list_words.append(l[0])\n",
    "list_words.append(word_to_look_up_final_score)\n",
    "\n",
    "nwords = len(list_words)\n",
    "nfeatures = model.transform(word_to_look_up_final_score).array.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================\n",
      "Number of total posts processed:  2001\n",
      "=================================================\n",
      "Number of words in the model: 1027\n",
      "=================================================\n",
      "Number of features per word:  100\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=================================================\")\n",
    "print(\"Number of total posts processed: \", totalposts)\n",
    "#print(\"=================================================\")\n",
    "#print(\"Number of filtered posts used: \", twcount)\n",
    "print(\"=================================================\")\n",
    "print(\"Number of words in the model:\", nwords)\n",
    "print(\"=================================================\")\n",
    "print(\"Number of features per word: \", nfeatures)\n",
    "print(\"=================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct the feature matrix, each row is asociated to each word in list_words\n",
    "feature_matrix = []\n",
    "found_words = 0\n",
    "for word in list_words:\n",
    "    found_words = total_words + 1\n",
    "    feature_matrix.append(model.transform(word).array)\n",
    "    update_progress(found_words, 0, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('1k_Gab_ai_posts_W2Vmatrix.npy',feature_matrix)\n",
    "np.save('1k_Gab_ai_posts_WordList.npy',list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " num_of_clusters = int(math.floor(math.sqrt(float(nwords)/2)))\n",
    "# Clusters ~ sqrt(n/2) is a fast approx according to : http://infolab.stanford.edu/~ullman/mmds/ch7.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature_Matrix = np.load('Gab_ai_posts_W2Vmatrix.npy')    # reads model generated by Word2Vec\n",
    "words = np.load('Gab_ai_posts_WordList.npy')    # reads list of words\n",
    "Featshape = Feature_Matrix.shape\n",
    "\n",
    "  \n",
    "## K-means clustering with Spark  \n",
    "maxiters=100\n",
    "clusters = KMeans.train(Feature_Matrix, k = num_of_clusters, maxIterations = maxiters) \n",
    "\n",
    "## Getting Cluster Labels for each Word and saving to a numpy file\n",
    "labels =  Feature_Matrix.map(lambda point: clusters.predict(point)) # add labels to each vector (word)\n",
    "list_labels = labels.collect()\n",
    "np.save('k_Clusters.npy',list_labels)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Size of the Word2vec matrix (words, features) is: \", Featshape)\n",
    "print(\"=\"*70)\n",
    "print(\"Number of clusters used: \", num_of_clusters)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature_Matrix = np.load('Gab_ai_posts_W2Vmatrix.npy')    # reads model generated by Word2Vec\n",
    "words = np.load('Gab_ai_posts_WordList.npy')    # reads list of words\n",
    "labels = np.load('/Users/jorgecastanon/Documents/github/w2v/mllib-scripts/myClusters.npy')\n",
    "\n",
    "Nw = words.shape[0]                # total number of words\n",
    "ind_star = np.where(word == words) # find index of the chosen word\n",
    "wstar = Feat[ind_star,:][0][0]     # vector corresponding to the chosen 'word'\n",
    "nwstar = math.sqrt(np.dot(wstar,wstar)) # norm of vector corresponding to the chosen 'word'\n",
    "\n",
    "dist = np.zeros(Nw) # initialize vector of distances\n",
    "i = 0\n",
    "for w in Feat: # loop to compute cosine distances \n",
    "    den = math.sqrt(np.dot(w,w))*nwstar  # compute denominator of cosine distance\n",
    "    dist[i] = abs( np.dot(wstar,w) ) / den   # compute cosine distance to each word\n",
    "    i = i + 1\n",
    "\n",
    "indexes = np.argpartition(dist,-(nwords+1))[-(nwords+1):]\n",
    "di = []\n",
    "for j in range(nwords+1):\n",
    "    di.append(( words[indexes[j]], dist[indexes[j]], labels[indexes[j]] ) )\n",
    "\n",
    "result = pd.DataFrame(di, columns = [\"word\",\"similarity\",\"cluster\"])\n",
    "return result.iloc[::-1] # order results from closest to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxWordsVis = 15\n",
    "\n",
    "Feat = np.load('Gab_ai_posts_W2Vmatrix.npy')  \n",
    "words = np.load('Gab_ai_posts_WordList.npy')\n",
    "# to rdd, avoid this with big matrices by reading them directly from hdfs\n",
    "Feat = sc.parallelize(Feat) \n",
    "Feat = Feat.map(lambda vec: (Vectors.dense(vec),))\n",
    "# to dataframe\n",
    "dfFeat = sqlContext.createDataFrame(Feat,[\"features\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFeat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfComp = model.transform(dfFeat).select(\"pcaFeatures\")\n",
    "# get the first two components to lists to be plotted\n",
    "compX = dfComp.map(lambda vec: vec[0][0]).take(maxWordsVis)\n",
    "compY = dfComp.map(lambda vec: vec[0][1]).take(maxWordsVis)\n",
    "compZ = dfComp.map(lambda vec: vec[0][2]).take(maxWordsVis)\n",
    "return compX, compY, compZ, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pyspark.__version()__) #\n",
    "#print(pyspark.version()) #\n",
    "import pyspark \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "result = np.array(dfFeat.select('features').collect())\n",
    "result = np.reshape(result,(1027,100))\n",
    "\n",
    "pca.fit(result)\n",
    "model = pca.transform(result)\n",
    "number_of_words = model.shape[0]\n",
    "assert number_of_words == len(words)\n",
    "print(model.shape)\n",
    "print(model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#%matplotlib qt5\n",
    "fs=20 #fontsize\n",
    "w = words[0:number_of_words]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "height = 10\n",
    "width = 10\n",
    "fig.set_size_inches(width, height)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    ax.scatter(model[i, 0], model[i, 1], model[i, 2], color='red', marker='o', edgecolors='black')\n",
    "    ax.text(model[i, 0], model[i, 1], model[i, 2], word)\n",
    "    #plt.scatter(model[i, 0], model[i, 1], color='red', marker='o', edgecolors='black')\n",
    "    \n",
    "for angle in range(0, 360):\n",
    "    ax.view_init(30, angle)\n",
    "    ax.draw()\n",
    "    ax.pause(.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "\n",
    "for word_num , word in enumerate(words):\n",
    "    for vector_num , vector in enumerate(model[0]):\n",
    "        word_label = words[word_num]\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
