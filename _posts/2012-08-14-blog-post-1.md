
# <center>Understanding Word2Vec with PySpark</center>
<center>Gabriel Fair</center>


<div style="text-align: right"><p style='text-align: right;'> <a href="https://github.com/gabefair/gabefair.github.io/blob/master/files/Spark-RDDs_With-Word2Vec.ipynb"> [This article is avaiable as a Jupyter Notebook](https://github.com/gabefair/gabefair.github.io/blob/master/files/Spark-RDDs_With-Word2Vec.ipynb) </a></p></div>


Today we are going to look at how Word2Vec incorporates word embeddings to create a numeric vectors to represent meaning of words. This is an important part of natural language processing (NLP). The goal of NLP is to extract meaning from human language, often this is provided in the form of text. And this meaning can be found in many components of language.

### Some components of language
  - Pragmatics
  - Semantics
  - Syntax
  - Morphology
  - Phonology

## Dataset 

I am using a Gab.ai dataset of posts submitted to the social platform. Gab.ai prides itself on the values of “free speech” and a lack of censorship. As a result it has become known for attracting trolls, bots, and the socially maligned. Comments and posts made to this site are notorious for being extreme and hate laden. I have collected over 28 million posts and will use a 1 million post sample to train a skip-grams variant of the word2vec word embedding model. The goal is to identify the proximity of two related words in a vector space.

## Distributional Semantic Models
Word embeddings are word representation algorithms used in an NLP. Word embeddings are a subclass of distributional semantic models because they rely on the distributional hypothesis. The distributional hypothesis, created by Zellig Harris in his 1956 paper [“Distributional structure”](http://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520) , is assumption that words in the same context tend to proport similar meanings and occur near each other. And thus synonyms have similar representations in a collection of texts. Word embeddings are represented as vector values created as a result of a neural network. 




<!---
<img style="display: block; margin: auto;" alt="photo" src="{{ site.baseurl }}/images/image.jpg">
-->

### We are going to start with some imports that we will need later [1]


```python
import os, sys, codecs, json, datetime

from time import time
import pyspark
print("pyspark version and install location: " + str(sc.version))
print(str(pyspark.version))
print("Location of code " + str(os.getcwd()))

from pyspark.mllib.feature import Word2Vec as Word2Vec #https://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html#word2vec
from pyspark.mllib.clustering import KMeans as KMeans
from pyspark.mllib.linalg import Vectors as Vectors

#https://spark.apache.org/docs/2.2.0/ml-features.html#word2vec
from pyspark.ml.feature import Word2Vec as Word2Vec2 #https://spark.apache.org/docs/2.2.0/ml-feature-extraction.html#word2vec
from pyspark.ml.clustering import KMeans as KMeans2
from pyspark.ml.linalg import Vectors as Vectors2

#from pyspark.ml.feature import PCA

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import math

from IPython.display import display
#This jupyter notebook started off with code from https://github.com/castanan/w2v [1] 
```

    pyspark version and install location: 2.4.0
    <module 'pyspark.version' from '/opt/spark-2.4.0-bin-hadoop2.7/python/pyspark/version.py'>
    Location of code /home/gfair/pyspark_word_embeddings


### Determine which version of scikit-learn learn we are using. 
#### [As of version 0.20.0 scikit-learn supports Pandas dataframes. ](https://medium.com/dunder-data/from-pandas-to-scikit-learn-a-new-exciting-workflow-e88e2271ef62)


```python
import sklearn
print("sklearn's version is: " + str(sklearn.__version__))
print("My python version is: "+ str(sys.version))
```

    sklearn's version is: 0.20.1
    My python version is: 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) 
    [GCC 7.2.0]



```python
#A method to monitor progress
def update_progress(current_progress, total, current):
    text = str(current_progress) + "/" + str(total) + " At: " + str(current)
    sys.stdout.write('\r' + text)
    sys.stdout.flush()
```

### Input 1 Million gab.ai posts


```python
path_to_text_data = '1mill_posts_unique_body_only.csv'
#filter_words = 'filter_word.csv' #Needs to one word per line

post_data = sc.textFile(path_to_text_data)
totalposts = post_data.count()
print("There are posts total: " + str(totalposts) )
```

    There are posts total: 1000001


#### Example gab.ai posts


```python
for i, y in enumerate(post_data.collect()):
    print(y)
    if i >5:
        break
```

    data.body
    "Probably because I see the faint hint of 'horns' holding that halo up... "
    https://youtu.be/YMQRFT4bZuc
    http://www.epochtimes.de/politik/europa/zahl-der-toten-nach-londoner-hochhausbrand-auf-79-gestiegen-2-a2146594.html
    https://t.co/LTMBeXvHrC
    "Ps 37:14 Die Gottlosen ziehen das Schwert aus und spannen ihren Bogen, daß sie fällen den Elenden und Armen und schlachten die Frommen.\nPs 37:15 Aber ihr Schwert wird in ihr Herz gehen, und ihr Bogen wird zerbrechen.\n\n"
    At least 25 killed in airstrike on market in Yemen – reports\nhttps://www.rt.com/news/392838-saudi-yemen-market-airstrike/ #saudiarabia #yemen


### Clean the text


```python
# clean characters by removing some characters and transform text to lower case
posts_RDD = post_data.map(lambda x: x.replace(";"," ").replace("\n"," ").replace("\r"," ").replace(":"," ").replace('"',' ').replace('-',' ').replace(',',' ').replace('.',' ').lower())

# tokenize into separate words
posts_RDD = posts_RDD.map(lambda row: row.split(" ")) 

```

#### Example after cleaning


```python
for i, y in enumerate(posts_RDD.collect()):
    print(y) #There is no need to worry about the blank string elements, they will be ignored anyway by the tokenizer
    if i >5:
        break
```

    ['data', 'body']
    ['', 'probably', 'because', 'i', 'see', 'the', 'faint', 'hint', 'of', "'horns'", 'holding', 'that', 'halo', 'up', '', '', '', '', '']
    ['https', '//youtu', 'be/ymqrft4bzuc']
    ['http', '//www', 'epochtimes', 'de/politik/europa/zahl', 'der', 'toten', 'nach', 'londoner', 'hochhausbrand', 'auf', '79', 'gestiegen', '2', 'a2146594', 'html']
    ['https', '//t', 'co/ltmbexvhrc']
    ['', 'ps', '37', '14', 'die', 'gottlosen', 'ziehen', 'das', 'schwert', 'aus', 'und', 'spannen', 'ihren', 'bogen', '', 'daß', 'sie', 'fällen', 'den', 'elenden', 'und', 'armen', 'und', 'schlachten', 'die', 'frommen', '\\nps', '37', '15', 'aber', 'ihr', 'schwert', 'wird', 'in', 'ihr', 'herz', 'gehen', '', 'und', 'ihr', 'bogen', 'wird', 'zerbrechen', '\\n\\n', '']
    ['at', 'least', '25', 'killed', 'in', 'airstrike', 'on', 'market', 'in', 'yemen', '–', 'reports\\nhttps', '//www', 'rt', 'com/news/392838', 'saudi', 'yemen', 'market', 'airstrike/', '#saudiarabia', '#yemen']


## Building a Vector
To use the distributional hypothesis to build a vector, we have to choose what words being near each other means to us. This value of “nearness” is known as a window. In the image below, taken from [Chris McCormick’s tutorial](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/), the target word is highlighted in blue, and the window shown around it as being two words away from the target.
This means the window size is equal to 2.
<img style="display: block; margin: auto;" alt="photo" src="http://mccormickml.com/assets/word2vec/training_data.png">

Word pairs are created between the target word and  all other words in the window which can extend forwards or backwards. The target is then moved to the next word and the process repeats. Some embedding models treat words to the left of the target word differently than words to the right. But for now we will treat them both equally.


These word pairs become the training samples for the model. These pairs are known as a one-hot vector. Currently they are in the form of (target word, context-word-in-window) and this will be used as the input for a simple 1 hidden layer neural network. The hidden layer has a pre-determined number of neurons that we specify as a hyper parameter. A hyper parameter is the number of hidden layer neurons has a large effect on the accacury and speed of the model’s runtime and 300 is widely used in practice since it was used by word2vec’s creators. This simple neural network is known as a Restricted Boltzmann Machine (RBM).

## Restricted Boltzmann Machines (RBMs)

<img style="display: block; margin: auto;" alt="photo" src="https://raw.githubusercontent.com/gabefair/gabefair.github.io/master/images/threelayers%5B1%5D.png">

In the image above, there are three columns that are known in discriptions of neural networks as layers. These diagrams show cause and effect between the layers of a neural network and are read from left-to-right. Each circle represents a neuron and is called a **node**. A node is where a calculation is preformed to determine if it will send a 0 or a 1 to a node in the next layer, which is to the right. This communication is known as **firing** and only happens in one direction, left-to-right. 


<img style="display: block; margin: auto;" alt="photo" src="https://raw.githubusercontent.com/gabefair/gabefair.github.io/master/images/three_layers_connected.png">


In our restricted boltzman machine, nodes are not linked to, or communicate with, other nodes within the same layer. This restriction gives the RBM its name. And every node in the input layer is linked to each node in the hidden layer. The nodes/neurons in the input layer are considered to be different neurons  in the hidden layer, hence why they are in different layers. 

I stress this point because this is known as a bipartite graph. But not just any bipartide graph, a complete bipartite graph because these two layers are fully linked. Note, that some texts call this a symmetrical bipartite graph. Also it is important to notice in the graphic above how the hidden layer has fewer nodes than the input or output layers. This is an important quality of RBMs as a feature known as dimensionality reduction.

When a RBM is inalitized, four things are determined in advance and thus are hard-coded into the construction of the neural network. This things are known as hyper parameters. 

  - Number of nodes in the input layer
  - Number of nodes in the hidden layer
  - Number of nodes in the output layer
  - The weights of the nodes in the hidden layer


With RBMs a special step happens when the hidden layer is created. Each node is randomlly assigned a weight. A wight is the power that node has on the nodes it is linked to in the next layer. This process of randomlly assigning weights is known as Stochastic Gradient Descent. It is called this because stocastic means “random” and these weights provide influence on the node in the next layer they are linked to. 

These weights are important to Word2Vec but unlike normal RBMs, Word2Vec does not randomly assign weights. Instead Word2Vec builts these weights over time while the neural network is fed our word pairs we created previously as input. This is known as training the neural network.

### Setup and train the neural network used by word2Vec


```python
#Word2Vec model param: wordIndex maps each word to an index, 
# which can retrieve the corresponding vector from wordVectors 
# param: wordVectors array of length numWords * vectorSize, vector corresponding to the word mapped with index i can be retrieved by the slice (i * vectorSize, i * vectorSize + vectorSize)

k = 300         # vector dimensionality (The number of nodes in the hidden layer)
wc = Word2Vec().setVectorSize(k) #Uses skip-gram model
```


```python
## train Word2vec
#dir(word2vec)
model = wc.fit(posts_RDD)

model_vectors = model.getVectors()
## Get the list of words in the w2v matrix
vocabsize = len(model_vectors)
print("Size of vocab list: " + str(vocabsize))
```

    Size of vocab list: 116568


<img style="display: block; margin: auto;" alt="photo" src="https://raw.githubusercontent.com/gabefair/gabefair.github.io/master/images/gabepic[1].png">

**Figure 4**: This image was adapted from [Chris McCormick's Word2Vec Tutorial - The Skip-Gram Model tutorial](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)

Above we created a word2vec neural network using pyspark's machine learning library. This function impliments the skip-gram model of word2vec. 

## Explaining skip-gram

#### Create vocab list from the words that have word2vec vectors


```python
vocab_list = {}
number_of_words = 0
for key in model_vectors:
    number_of_words += 1
    vocab_list[key] = model_vectors[key]
    update_progress(number_of_words, vocabsize, key)
print("Double checking the size of the vocab list: " + str(len(vocab_list.keys())))
```

    Double checking the size of the vocab list: 116568


#### Just the output vector for the word "trump"


```python
print(model_vectors['trump'])
```

    [0.012244515, 0.05669574, 0.4243817, -0.13005282, 0.14591245, 0.15754509, 0.12144023, -0.043377433, -0.036471862, -0.17795071, -0.15042417, 0.4285602, -0.16748007, -0.09618644, 0.07635299, 0.021112783, -0.1097202, 0.16649377, 0.31761286, 0.2781521, 0.26321766, -0.35739362, -0.17595355, -0.28173986, 0.2220869, 0.421465, 0.12334027, 0.17061687, -0.16097873, 0.1101991, 0.39143816, -0.10224187, 0.19060156, 0.06379647, -0.055479944, 0.30508712, -0.33523571, -0.3099334, 0.16205992, 0.23172502, 0.12932838, -0.25712037, -0.24778262, -0.41348562, 0.10876833, -0.095286794, -0.12277438, 0.08167293, 0.2416396, -0.29519707, 0.07202256, -0.03740526, -0.08972215, -0.03250894, -0.21824007, 0.04827257, -0.009086915, 0.18352096, -0.10135367, -0.47981852, -0.06576853, 0.021472175, 0.023349164, 0.05336668, -0.37836334, 0.08596835, -0.08231194, -0.09812828, 0.0058923, -0.06080334, 0.15352124, 0.2911331, 0.15038647, 0.15921666, 0.13570379, 0.09163106, 0.0093092015, 0.0024938602, 0.16191821, -0.116921216, 0.37449756, -0.37325835, 0.17355393, 0.22919315, -0.22791475, -0.12990569, 0.15548478, -0.16302991, 0.09529176, -0.124482594, 0.01942392, -0.18610963, -0.43775123, -0.2965226, -0.07572919, 0.2682866, 0.15111415, -0.03312072, 0.023581406, -0.035215713]



```python
print("Size of vector for the word 'looks': " + str(len(model_vectors['trump'])))
```

    Size of vector for the word 'looks': 300


Notice the word vector size equals the value of `k` that we chose above. This vector size is equal to the number of nodes in the hidden layer


```python
word_to_look_up_final_score = 'trump'
another_word_to_compare_it_against = 'president'

#Find synonyms of a word; do not include the word itself in results
word_cosine_similarity_arry_word1 = model.findSynonyms(word_to_look_up_final_score, vocabsize-1) #returns an array of (word, cosineSimilarity)
word_cosine_similarity_arry_word2 = model.findSynonyms(another_word_to_compare_it_against, vocabsize-1) #returns an array of (word, cosineSimilarity)

list_words = []
for l in word_cosine_similarity_arry_word1:
    list_words.append(l[0])
list_words.append(word_to_look_up_final_score)

nwords = len(list_words)
nfeatures = model.transform(word_to_look_up_final_score).array.shape[0]
```


```python
print("="*70)
print("Total posts processed: ", totalposts)
print("="*70)
print("Total words in the model:", nwords)
print("="*70)
print("Total features per word: ", nfeatures) #Determined by k above
print("="*70)
```

    =================================================
    Number of total posts processed:  2001
    =================================================
    Number of words in the model: 1027
    =================================================
    Number of features per word:  300
    =================================================



```python
## Construct the feature matrix, each row is asociated to each word in list_words
feature_matrix = []
found_words = 0

for word in list_words:
    found_words = found_words + 1
    feature_matrix.append(model.transform(word).array)
    update_progress(found_words, vocabsize, word)
```

    116568/116568 At: trumpalboxgtwitteredias\n\nupdateicheemistenkeial&utm_source=twitterre #followthewhiterabbit #magaviewsublicansin18\n\r\ngod


```python
np.save('1m_Gab_ai_posts_W2Vmatrix.npy',feature_matrix)
#np.save('1m_Gab_ai_posts_vocab_list.npy',vocab_list)
np.save('1m_Gab_ai_posts_WordList.npy',list_words)
```

## Now visualize the vector space using PCA and KMeans

Here I have to specify the number of clusters that Kmeans should use. A good approximation is to take the square root of half the number of words in the vocabulary list.


```python
 num_of_clusters = int(math.floor(math.sqrt(float(nwords)/2)))
# Clusters ~ sqrt(n/2) is a fast approx according to : http://infolab.stanford.edu/~ullman/mmds/ch7.pdf

```


```python
Feature_Matrix = np.load('1m_Gab_ai_posts_W2Vmatrix.npy')    # reads model generated by Word2Vec
words = np.load('1m_Gab_ai_posts_WordList.npy')    # reads list of words
Featshape = Feature_Matrix.shape

Feature_Matrix = sc.parallelize(Feature_Matrix)
  
## K-means clustering with Spark  
maxiters=100
clusters = KMeans.train(Feature_Matrix, k = num_of_clusters, maxIterations = maxiters) 

## Getting Cluster Labels for each Word and saving to a numpy file
labels =  Feature_Matrix.map(lambda point: clusters.predict(point)) # add labels to each vector (word)
list_labels = labels.collect()
np.save('1m_Gab_ai_posts_k_Clusters.npy',list_labels)
```

    ======================================================================
    Size of the Word2vec matrix (words, features) is:  (116568, 300)
    ======================================================================
    Number of clusters used:  241
    ======================================================================



```python
print("="*70)
print("Size of the Word2vec matrix (words, features) is: ", Featshape)
print("="*70)
print("Number of clusters used: ", num_of_clusters) #determined above by sqrt(nwords/2)
print("="*70)
print("The dimensions of the Word2Vec matrix: "+ str(Featshape))
```

    ======================================================================
    Size of the Word2vec matrix (words, features) is:  (116568, 300)
    ======================================================================
    Number of clusters used:  241
    ======================================================================
    The dimensions of the Word2Vec matrix: (116568, 300)



```python
word1 = 'trump'
word2 = 'maga'
```


```python
Feature_Matrix = np.load('1m_Gab_ai_posts_W2Vmatrix.npy')    # reads model generated by Word2Vec
words = np.load('1m_Gab_ai_posts_WordList.npy')    # list of all the words present in the 1m posts
labels = np.load('1m_Gab_ai_posts_k_Clusters.npy')


Nw = words.shape[0]                # total number of words
print("Third check for the total number of words: "+ str(Nw))
ind_star = np.where(word1 == words) # find index of the chosen word
print(ind_star)
```

    Third check for the total number of words: 116568
    (array([116567]),)



```python
wstar = Feature_Matrix[ind_star,:][0][0]     # vector corresponding to the chosen 'word'
print(wstar)

```

    [ 0.01224452  0.05669574  0.4243817  -0.13005282  0.14591245  0.15754509
      0.12144023 -0.04337743 -0.03647186 -0.17795071 -0.15042417  0.4285602
     -0.16748007 -0.09618644  0.07635299  0.02111278 -0.1097202   0.16649377
      0.31761286  0.27815211  0.26321766 -0.35739362 -0.17595355 -0.28173986
      0.22208691  0.42146501  0.12334027  0.17061687 -0.16097873  0.1101991
      0.39143816 -0.10224187  0.19060156  0.06379647 -0.05547994  0.30508712
     -0.33523571 -0.30993339  0.16205992  0.23172502  0.12932838 -0.25712037
     -0.24778262 -0.41348562  0.10876833 -0.09528679 -0.12277438  0.08167293
      0.2416396  -0.29519707  0.07202256 -0.03740526 -0.08972215 -0.03250894
     -0.21824007  0.04827257 -0.00908692  0.18352096 -0.10135367 -0.47981852
     -0.06576853  0.02147217  0.02334916  0.05336668 -0.37836334  0.08596835
     -0.08231194 -0.09812828  0.0058923  -0.06080334  0.15352124  0.29113311
      0.15038647  0.15921666  0.13570379  0.09163106  0.0093092   0.00249386
      0.16191821 -0.11692122  0.37449756 -0.37325835  0.17355393  0.22919315
     -0.22791475 -0.12990569  0.15548478 -0.16302991  0.09529176 -0.12448259
      0.01942392 -0.18610963 -0.43775123 -0.29652259 -0.07572919  0.26828659
      0.15111415 -0.03312072  0.02358141 -0.03521571]



```python
?nwstar = math.sqrt(np.dot(wstar,wstar)) # by taking the square root of the dot product between the 
print(nwstar)
```

    2.034438810103313


### Find cosine simularity


```python


dist = np.zeros(Nw) # initialize vector of distances
i = 0
for w in Feature_Matrix: # loop to compute cosine distances 
    den = math.sqrt(np.dot(w,w))*nwstar  # compute denominator of cosine distance
    dist[i] = abs( np.dot(wstar,w) ) / den   # compute cosine distance to each word
    i = i + 1
```


```python
nwords = 100
indexes = np.argpartition(dist,-(nwords+1))[-(nwords+1):]
di = []
for counter in range(nwords+1):
    di.append(( words[indexes[counter]], dist[indexes[counter]], labels[indexes[counter]] ) )
print(di[2])
```

    ('inauguration/', 0.5486706985071742, 112)



```python
unsorted_result = pd.DataFrame(di, columns = ["word","similarity","cluster"])
print(unsorted_result)
```

                                   word  similarity  cluster
    0                     trump\n\nhttp    0.547598      218
    1                      nevertrumper    0.547763       13
    2                     inauguration/    0.548671      112
    3              statements/president    0.548829       17
    4                       trump\nhttp    0.549403      218
    5                          streep’s    0.550304       35
    6                                 j    0.550414      112
    7        com/2017/06/28/condoleezza    0.551023      187
    8                             call/    0.551359       35
    9                      ‘resistance’    0.551726       35
    10                      haiti\nhttp    0.551846        0
    11                     house/356849    0.552361       35
    12                   trump\n\ntrump    0.553146       35
    13                       com/donald    0.553214       83
    14                          pledge/    0.553681      118
    15      office/2017/06/29/statement    0.553720       82
    16                      \npresident    0.555016      112
    17   com/california/2017/01/07/dear    0.557056        0
    18               statements/remarks    0.559158      144
    19      office/2017/06/19/statement    0.560339      112
    20         com/2017/07/03/president    0.561896       17
    21          com/2018/01/13/pandoras    0.562444      112
    22                     erin'strump®    0.562768      118
    23                          sexist/    0.563230      118
    24                              djt    0.563693      112
    25                 com/2017/06/mika    0.563695       17
    26                          remark/    0.564070       35
    27            com/2018/01/president    0.564370      218
    28              com/2017/10/winning    0.564972        0
    29                       'president    0.565560       35
    ..                              ...         ...      ...
    71                  schumershutdown    0.616409       35
    72                        netanyahu    0.617232      181
    73                         trump?\n    0.618699       35
    74                           feeley    0.623910       35
    75                       ‘shithole’    0.624290       13
    76                            shole    0.627038      221
    77                            obama    0.627831       93
    78                          peegate    0.628730       35
    79                             soci    0.632695       35
    80                         tv/watch    0.633391      144
    81                    com/appalling    0.634154      118
    82                            pence    0.636276      112
    83                           somers    0.638150      118
    84          video/?utm_medium=email    0.641376       35
    85                         remarks/    0.643584       13
    86                    html\n\ntrump    0.650845       35
    87                           peotus    0.656724       35
    88                           trump?    0.660687      112
    89                 winner\r\nsearch    0.663887      118
    90                       unverified    0.666894      128
    91                           trump/    0.671885      112
    92                    clintonrussia    0.680471       35
    93                           trumps    0.686771      112
    94                        president    0.687781      112
    95                          trump’s    0.688237      112
    96                           donald    0.694213      112
    97                            potus    0.705011      112
    98                            elect    0.722010      112
    99                          trump's    0.729306      112
    100                           trump    1.000000      112
    
    [101 rows x 3 columns]



```python
ranked_results = unsorted_result.iloc[::-1] # order results from closest to chosen word
print(ranked_results)
```

                                   word  similarity  cluster
    100                           trump    1.000000      112
    99                          trump's    0.729306      112
    98                            elect    0.722010      112
    97                            potus    0.705011      112
    96                           donald    0.694213      112
    95                          trump’s    0.688237      112
    94                        president    0.687781      112
    93                           trumps    0.686771      112
    92                    clintonrussia    0.680471       35
    91                           trump/    0.671885      112
    90                       unverified    0.666894      128
    89                 winner\r\nsearch    0.663887      118
    88                           trump?    0.660687      112
    87                           peotus    0.656724       35
    86                    html\n\ntrump    0.650845       35
    85                         remarks/    0.643584       13
    84          video/?utm_medium=email    0.641376       35
    83                           somers    0.638150      118
    82                            pence    0.636276      112
    81                    com/appalling    0.634154      118
    80                         tv/watch    0.633391      144
    79                             soci    0.632695       35
    78                          peegate    0.628730       35
    77                            obama    0.627831       93
    76                            shole    0.627038      221
    75                       ‘shithole’    0.624290       13
    74                           feeley    0.623910       35
    73                         trump?\n    0.618699       35
    72                        netanyahu    0.617232      181
    71                  schumershutdown    0.616409       35
    ..                              ...         ...      ...
    29                       'president    0.565560       35
    28              com/2017/10/winning    0.564972        0
    27            com/2018/01/president    0.564370      218
    26                          remark/    0.564070       35
    25                 com/2017/06/mika    0.563695       17
    24                              djt    0.563693      112
    23                          sexist/    0.563230      118
    22                     erin'strump®    0.562768      118
    21          com/2018/01/13/pandoras    0.562444      112
    20         com/2017/07/03/president    0.561896       17
    19      office/2017/06/19/statement    0.560339      112
    18               statements/remarks    0.559158      144
    17   com/california/2017/01/07/dear    0.557056        0
    16                      \npresident    0.555016      112
    15      office/2017/06/29/statement    0.553720       82
    14                          pledge/    0.553681      118
    13                       com/donald    0.553214       83
    12                   trump\n\ntrump    0.553146       35
    11                     house/356849    0.552361       35
    10                      haiti\nhttp    0.551846        0
    9                      ‘resistance’    0.551726       35
    8                             call/    0.551359       35
    7        com/2017/06/28/condoleezza    0.551023      187
    6                                 j    0.550414      112
    5                          streep’s    0.550304       35
    4                       trump\nhttp    0.549403      218
    3              statements/president    0.548829       17
    2                     inauguration/    0.548671      112
    1                      nevertrumper    0.547763       13
    0                     trump\n\nhttp    0.547598      218
    
    [101 rows x 3 columns]


## Visualize Words using PCA


```python
maxWordsVis = 10

Feature_Matrix = np.load('1m_Gab_ai_posts_W2Vmatrix.npy')
words = np.load('1m_Gab_ai_posts_WordList.npy')    
labels = np.load('1m_Gab_ai_posts_k_Clusters.npy')

# to rdd, avoid this with big matrices by reading them directly from hdfs
Feature_Matrix = sc.parallelize(Feature_Matrix) 
Feature_Matrix = Feature_Matrix.map(lambda vec: (Vectors.dense(vec),))
print(Feature_Matrix)

i = 0
for i, y in enumerate(Feature_Matrix.collect()):
    print(y)
    if i > 1:
        break

```

    PythonRDD[530] at RDD at PythonRDD.scala:53
    (DenseVector([-0.0341, -0.1206, 0.1185, -0.382, 0.4706, 0.1678, 0.0845, -0.065, -0.1319, -0.0437, -0.1109, 0.1578, -0.0601, 0.0576, -0.1105, 0.128, -0.1189, 0.1832, 0.3394, 0.2208, 0.1529, -0.1838, -0.2553, -0.3699, 0.3997, 0.2381, 0.0265, 0.0749, -0.108, -0.1738, 0.1878, 0.0474, 0.1689, -0.1546, -0.1569, 0.5899, -0.3813, -0.3174, 0.2272, 0.258, -0.005, 0.1246, -0.2113, -0.357, 0.114, 0.1293, 0.0331, 0.1564, 0.0522, -0.227, 0.1024, 0.0219, -0.2599, 0.1272, -0.2558, 0.2002, -0.076, 0.1791, -0.0599, -0.1397, -0.0447, -0.1395, -0.1713, 0.1225, -0.0783, 0.1606, -0.1383, -0.4474, 0.1089, -0.0278, 0.2616, 0.2803, 0.0016, 0.0748, 0.1426, -0.1248, -0.1004, 0.0017, -0.0174, 0.0089, 0.1502, -0.4128, 0.081, 0.2939, -0.4873, -0.2055, 0.0895, 0.0581, -0.0087, -0.0582, -0.2119, -0.1542, -0.2071, -0.2389, -0.0549, 0.2859, 0.027, 0.0035, 0.1365, -0.0017]),)
    (DenseVector([0.2082, -0.2334, 0.0597, -0.1223, 0.4008, 0.3984, -0.1158, -0.0427, -0.1782, -0.2688, 0.1096, 0.4766, -0.3821, 0.067, 0.1741, -0.032, -0.2266, 0.3974, 0.2718, 0.1744, 0.3785, -0.0068, -0.2948, -0.3965, 0.2492, 0.5165, 0.5816, 0.3896, -0.2028, 0.0995, 0.3021, -0.1128, 0.5759, 0.1909, -0.0593, 0.3837, -0.2743, -0.4606, 0.4966, 0.3191, 0.1354, -0.0104, -0.6013, -0.4097, 0.0437, -0.188, 0.6398, 0.113, 0.234, -0.1382, 0.0658, 0.1504, -0.3535, 0.3883, -0.5367, 0.1201, 0.3473, 0.3666, 0.1147, -0.4638, -0.2973, 0.1284, 0.0975, -0.078, -0.4722, 0.0949, 0.0559, -0.357, -0.0835, 0.0279, 0.2257, 0.3806, 0.2208, 0.1474, 0.2185, 0.1013, -0.0425, 0.0929, 0.5632, -0.2781, 0.4155, -0.3132, 0.2167, 0.0321, -0.5692, 0.0999, 0.2853, -0.1351, 0.2881, -0.343, -0.2616, -0.1551, -0.4003, -0.103, 0.3845, 0.0562, -0.1499, 0.2119, 0.2213, -0.1253]),)
    (DenseVector([0.013, -0.1758, 0.0929, -0.0854, 0.2757, 0.243, -0.0475, -0.1093, 0.0117, -0.0595, 0.0096, 0.0961, -0.1594, -0.2598, 0.2644, 0.059, -0.1131, 0.1345, 0.0659, 0.1604, -0.0492, -0.2054, -0.118, -0.254, 0.2234, 0.2872, 0.3486, 0.1503, -0.2318, 0.0894, 0.3393, -0.0969, 0.0044, -0.1604, -0.0971, 0.184, -0.6559, -0.0521, 0.0824, 0.1884, 0.1477, -0.2366, -0.4829, -0.126, 0.2768, -0.1333, 0.2615, 0.1881, 0.1633, -0.105, 0.0528, -0.0539, -0.1955, -0.0366, -0.2502, 0.0554, 0.2813, 0.1423, -0.1984, -0.2994, -0.0314, -0.101, 0.0343, 0.0688, -0.304, 0.3232, -0.2449, -0.2106, -0.1559, -0.1766, 0.1844, 0.3145, -0.1288, -0.0225, 0.0361, 0.103, 0.0258, 0.1883, -0.0414, -0.0924, 0.0944, -0.2402, 0.0643, 0.1294, -0.42, -0.0257, -0.0824, -0.0361, 0.1243, -0.0896, -0.0096, -0.3654, -0.1955, -0.0817, -0.0142, 0.1503, 0.179, -0.0105, -0.0497, -0.2263]),)



```python
# to dataframe
dfFeat = sqlContext.createDataFrame(Feature_Matrix,["features"])
print(dfFeat.head())
```

    Row(features=DenseVector([-0.0341, -0.1206, 0.1185, -0.382, 0.4706, 0.1678, 0.0845, -0.065, -0.1319, -0.0437, -0.1109, 0.1578, -0.0601, 0.0576, -0.1105, 0.128, -0.1189, 0.1832, 0.3394, 0.2208, 0.1529, -0.1838, -0.2553, -0.3699, 0.3997, 0.2381, 0.0265, 0.0749, -0.108, -0.1738, 0.1878, 0.0474, 0.1689, -0.1546, -0.1569, 0.5899, -0.3813, -0.3174, 0.2272, 0.258, -0.005, 0.1246, -0.2113, -0.357, 0.114, 0.1293, 0.0331, 0.1564, 0.0522, -0.227, 0.1024, 0.0219, -0.2599, 0.1272, -0.2558, 0.2002, -0.076, 0.1791, -0.0599, -0.1397, -0.0447, -0.1395, -0.1713, 0.1225, -0.0783, 0.1606, -0.1383, -0.4474, 0.1089, -0.0278, 0.2616, 0.2803, 0.0016, 0.0748, 0.1426, -0.1248, -0.1004, 0.0017, -0.0174, 0.0089, 0.1502, -0.4128, 0.081, 0.2939, -0.4873, -0.2055, 0.0895, 0.0581, -0.0087, -0.0582, -0.2119, -0.1542, -0.2071, -0.2389, -0.0549, 0.2859, 0.027, 0.0035, 0.1365, -0.0017]))



```python
from sklearn.decomposition import PCA
pca = PCA(n_components=3)
result = np.array(dfFeat.select('features').collect())
result = np.reshape(result,Featshape)

pca.fit(result)
model = pca.transform(result)
number_of_words = model.shape[0]
assert number_of_words == len(words)
print(model.shape)
print(model[0])
```

    (116568, 3)
    [-0.60580882  0.22091147 -0.21770608]



```python
for i, word in enumerate(words):
    ax.scatter(model[i, 0], model[i, 1], color='red', marker='o', edgecolors='black')
    ax.text(model[i, 0], model[i, 1], model[i, 2], word)
    plt.scatter(model[i, 0], model[i, 1], color='red', marker='o', edgecolors='black')
    if(i > 50):
        break
```


![png](output_61_0.png)



```python
counter = 0
i = 0
plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')
for counter in range(nwords+1):
    word_lable = ranked_results.iloc[counter][0]
    cosine_sim = ranked_results.iloc[counter][1]
    assigned_cluster = ranked_results.iloc[counter][2]
    
    plt.scatter(dist[indexes[counter]], labels[indexes[counter]], color='red', marker='o', edgecolors='black')
    plt.annotate(word_lable, (cosine_sim, assigned_cluster))
    if(i > 10):
        break
    
plt.show()
```


![png](output_62_0.png)



```python
ranked_results.iloc[i][2]
```




    112




```python
import seaborn as sns

#sns.set(style='ticks', context='talk')
i = 0
for i in range(Featshape[0]):
    if(i == 0):
        continue #skip the header
    word_lable = ranked_results.iloc[i][0]
    cosine_sim = ranked_results.iloc[i][1]
    assigned_cluster = ranked_results.iloc[i][2]
    print(assigned_cluster)
    
    sns.stripplot(x=cosine_sim.astype(np.float64), y=assigned_cluster, jitter=0.2)
    if(i > 50): #We don't want too many points, just the top words according to cosine simularity
        break
sns.despine()
```

    112



    ---------------------------------------------------------------------------

    AttributeError                            Traceback (most recent call last)

    /opt/anaconda/lib/python3.6/site-packages/seaborn/utils.py in categorical_order(values, order)
        507             try:
    --> 508                 order = values.cat.categories
        509             except (TypeError, AttributeError):


    AttributeError: 'numpy.float64' object has no attribute 'cat'

    
    During handling of the above exception, another exception occurred:


    AttributeError                            Traceback (most recent call last)

    /opt/anaconda/lib/python3.6/site-packages/seaborn/utils.py in categorical_order(values, order)
        510                 try:
    --> 511                     order = values.unique()
        512                 except AttributeError:


    AttributeError: 'numpy.float64' object has no attribute 'unique'

    
    During handling of the above exception, another exception occurred:


    TypeError                                 Traceback (most recent call last)

    <ipython-input-238-dbdd57588d9e> in <module>()
         11     print(assigned_cluster)
         12 
    ---> 13     sns.stripplot(x=cosine_sim.astype(np.float64), y=assigned_cluster, jitter=0.2)
         14     if(i > 50): #We don't want too many points, just the top words according to cosine simularity
         15         break


    /opt/anaconda/lib/python3.6/site-packages/seaborn/categorical.py in stripplot(x, y, hue, data, order, hue_order, jitter, dodge, orient, color, palette, size, edgecolor, linewidth, ax, **kwargs)
       2587 
       2588     plotter = _StripPlotter(x, y, hue, data, order, hue_order,
    -> 2589                             jitter, dodge, orient, color, palette)
       2590     if ax is None:
       2591         ax = plt.gca()


    /opt/anaconda/lib/python3.6/site-packages/seaborn/categorical.py in __init__(self, x, y, hue, data, order, hue_order, jitter, dodge, orient, color, palette)
       1130                  jitter, dodge, orient, color, palette):
       1131         """Initialize the plotter."""
    -> 1132         self.establish_variables(x, y, hue, data, orient, order, hue_order)
       1133         self.establish_colors(color, palette, 1)
       1134 


    /opt/anaconda/lib/python3.6/site-packages/seaborn/categorical.py in establish_variables(self, x, y, hue, data, orient, order, hue_order, units)
        197 
        198                 # Get the order on the categorical axis
    --> 199                 group_names = categorical_order(groups, order)
        200 
        201                 # Group the numeric data


    /opt/anaconda/lib/python3.6/site-packages/seaborn/utils.py in categorical_order(values, order)
        511                     order = values.unique()
        512                 except AttributeError:
    --> 513                     order = pd.unique(values)
        514                 try:
        515                     np.asarray(values).astype(np.float)


    /opt/anaconda/lib/python3.6/site-packages/pandas/core/algorithms.py in unique(values)
        365     htable, _, values, dtype, ndtype = _get_hashtable_algo(values)
        366 
    --> 367     table = htable(len(values))
        368     uniques = table.unique(values)
        369     uniques = _reconstruct_data(uniques, dtype, original)


    TypeError: len() of unsized object



```python

```




    144




```python


```


```python
print(ranked_results.iloc[1][0])
```

    trump's



```python

```

    (116568, 3)
    [-0.60580882  0.22091106 -0.21771847]



```python
%matplotlib inline
#%matplotlib qt5
fs=20 #fontsize
w = words[0:number_of_words]
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
height = 10
width = 10
fig.set_size_inches(width, height)

```


```python
number_of_words = 1027#vocab_count
```


```python

```


![png](output_71_0.png)



```python
%matplotlib inline
fs=20 #fontsize
w = words[0:maxWordsVis]
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

height = 10
width = 10
fig.set_size_inches(width, height)

ax.scatter(compX, compY, compZ, color='red', s=100, marker='o', edgecolors='black')
for i, txt in enumerate(w):
    ax.text(compX[i],compY[i],compZ[i], '%s' % (txt), size=8, zorder=1, color='k')
ax.set_xlabel('1st. Component', fontsize=fs)
ax.set_ylabel('2nd. Component', fontsize=fs)
ax.set_zlabel('3rd. Component', fontsize=fs)
ax.set_title('Visualization of Word2Vec via PCA', fontsize=fs)
ax.grid(True)
plt.show()
```


    ---------------------------------------------------------------------------

    NameError                                 Traceback (most recent call last)

    <ipython-input-140-1824bc955f1c> in <module>()
          9 fig.set_size_inches(width, height)
         10 
    ---> 11 ax.scatter(compX, compY, compZ, color='red', s=100, marker='o', edgecolors='black')
         12 for i, txt in enumerate(w):
         13     ax.text(compX[i],compY[i],compZ[i], '%s' % (txt), size=8, zorder=1, color='k')


    NameError: name 'compX' is not defined



![png](output_72_1.png)



```python
%matplotlib inline
fs=20 #fontsize
w = words[0:maxWordsVis]
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

height = 10
width = 10
fig.set_size_inches(width, height)

ax.scatter(compX, compY, compZ, color='red', s=100, marker='o', edgecolors='black')
for i, txt in enumerate(w):
    ax.text(compX[i],compY[i],compZ[i], '%s' % (txt), size=8, zorder=1, color='k')
ax.set_xlabel('1st. Component', fontsize=fs)
ax.set_ylabel('2nd. Component', fontsize=fs)
ax.set_zlabel('3rd. Component', fontsize=fs)
ax.set_title('Visualization of Word2Vec via PCA', fontsize=fs)
ax.grid(True)
plt.show()
```


```python
import matplotlib.pyplot as plt

from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(1, figsize=(4, 3))
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)

for word_num , word in enumerate(words):
    for vector_num , vector in enumerate(model[0]):
        word_label = words[word_num]
       
```


![png](output_74_0.png)


## References and Credits


```python
[1] Jorge Castanon at https://github.com/castanan/w2v
```
